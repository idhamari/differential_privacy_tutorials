{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPzdZWKA77QByR076PTa3sg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idhamari/differential_privacy_tutorials/blob/main/Differential_Privacy_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In these tutorial, I will try to explain Differential Privacy (DP) in a practical way with examples using simple python scripts. I will try to add more details, examples, and scripts whenever i find the time, so please consider this work in progress.\n",
        "\n",
        "**Differential Privacy** is a framework for measuring the privacy guarantees provided by an algorithm. Its central concept is a \"privacy budget\", a measure of how much a person's privacy is affected by participating in the dataset.\n",
        "\n",
        "Let's start with the theoretical part:\n",
        "\n",
        "**Randomized Algorithm:** An algorithm is randomized if its output contains a certain amount of randomness. In other words, given the same input, the output may not always be the same.\n",
        "\n",
        "**Neighboring Databases:** We say that two databases, D1 and D2, are neighboring databases if they differ by only one entry. This difference corresponds to the information of exactly one individual.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_wmnTO8hocVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epsilon-Differential Privacy (ϵ-DP):\n",
        "\n",
        "A randomized algorithm A satisfies ϵ-DP if for all datasets D1 and D2 that differ on a single element, and all sets of possible outputs S, the following inequality holds:\n",
        "\n",
        "Pr[A(D1) ∈ S] ≤ e^ϵ * Pr[A(D2) ∈ S]\n",
        "\n",
        "This inequality means that the presence or absence of a single database item does not significantly change the probability distribution of the algorithm's output.\n",
        "\n",
        "For simplicity, let's consider an example where we're trying to calculate the average age in a dataset, and we want to do it under differential privacy.\n"
      ],
      "metadata": {
        "id": "9BruRknAq-gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laplace Mechanism\n",
        "\n",
        "Here's a Python implementation of **differential privacy using the Laplace Mechanism**, one of the most common techniques for adding noise. The Laplace mechanism adds noise proportional to the L1 sensitivity of the query / function, divided by the privacy parameter epsilon.\n",
        "\n",
        "This code calculates the average age and adds noise to it. The amount of noise is determined by the sensitivity of the query (how much the result can change with the addition/removal of a single database entry) and the privacy parameter epsilon.\n",
        "\n",
        "Keep in mind that this is a basic example. Differential privacy can be complex, especially when considering different types of queries, complex data types, or when you want to limit the cumulative privacy loss from multiple queries. These advanced topics often involve techniques such as composition theorems, privacy amplification, or post-processing. For further understanding, I recommend you to refer to detailed textbooks and academic papers on the subject."
      ],
      "metadata": {
        "id": "FSgOrjW4rQPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Privacy parameter\n",
        "epsilon = 0.5\n",
        "\n",
        "# Our sensitive data\n",
        "ages = np.array([25, 30, 35, 40, 45])\n",
        "\n",
        "# The query we want to answer (average age)\n",
        "query_result = np.mean(ages)\n",
        "\n",
        "# Compute the L1 sensitivity of the query\n",
        "sensitivity = 1.0 / len(ages)\n",
        "\n",
        "# Add noise to the result of the query.\n",
        "# The amount of noise is sampled from a Laplace distribution\n",
        "noise = np.random.laplace(loc=0, scale=sensitivity/epsilon)\n",
        "\n",
        "# The final, differentially private result\n",
        "dp_result = query_result + noise\n",
        "\n",
        "print(f\"Query result: {query_result}\")\n",
        "print(f\"Differentially private result: {dp_result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8_i2PfPocne",
        "outputId": "6e4e3685-b626-4f13-b72e-f158bbd32427"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query result: 35.0\n",
            "Differentially private result: 35.28061752749318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (ε, δ)-differential privacy\n",
        "\n",
        "To handle the cumulative privacy loss from multiple queries, we introduce the concept of **(ε, δ)-differential privacy**. It extends ε-differential privacy by allowing the probability of a certain amount of additional privacy loss, governed by δ. A randomized algorithm is said to have (ε, δ)-differential privacy if it satisfies the ε-differential privacy condition with probability 1-δ.\n",
        "\n",
        "The privacy parameter ε controls the 'worst-case' privacy loss, and δ allows the algorithm to have a very small chance of additional privacy loss. Normally, δ is set to be a very small positive number.\n",
        "\n",
        "Let's extend our Python example by considering multiple queries, where each query is calculating the average age in a different way (e.g., average age of males and females). Here, we will simulate two queries:"
      ],
      "metadata": {
        "id": "dKM3muuipSJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Privacy parameters\n",
        "epsilon = 0.5\n",
        "delta = 0.01\n",
        "\n",
        "# Our sensitive data (ages of males and females)\n",
        "ages_males = np.array([25, 30, 35, 40, 45])\n",
        "ages_females = np.array([27, 32, 37, 42, 47])\n",
        "\n",
        "# Two queries we want to answer (average age for males and females)\n",
        "query_result_males = np.mean(ages_males)\n",
        "query_result_females = np.mean(ages_females)\n",
        "\n",
        "# Compute the L1 sensitivity of the queries\n",
        "sensitivity_males = 1.0 / len(ages_males)\n",
        "sensitivity_females = 1.0 / len(ages_females)\n",
        "\n",
        "# Add noise to the result of the queries.\n",
        "# The amount of noise is sampled from a Laplace distribution\n",
        "noise_males = np.random.laplace(loc=0, scale=sensitivity_males/epsilon)\n",
        "noise_females = np.random.laplace(loc=0, scale=sensitivity_females/epsilon)\n",
        "\n",
        "# The final, differentially private results\n",
        "dp_result_males = query_result_males + noise_males\n",
        "dp_result_females = query_result_females + noise_females\n",
        "\n",
        "print(f\"Query result for males: {query_result_males}\")\n",
        "print(f\"Differentially private result for males: {dp_result_males}\")\n",
        "print(f\"Query result for females: {query_result_females}\")\n",
        "print(f\"Differentially private result for females: {dp_result_females}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK5Vo4UppVbB",
        "outputId": "45e539f3-de9a-4585-8d6e-dec2f439a43d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query result for males: 35.0\n",
            "Differentially private result for males: 35.40560720277377\n",
            "Query result for females: 37.0\n",
            "Differentially private result for females: 36.57699435712488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above Python script, we applied the Laplace Mechanism to each query independently. This is fine if we have a single privacy budget that we can split across all queries. However, the privacy guarantee degrades when the same individual's data is used in multiple queries.\n",
        "\n",
        "To manage this, in practice, we use composition theorems to handle the cumulative privacy loss from multiple queries. Furthermore, more sophisticated mechanisms such as the Exponential Mechanism, Gaussian Mechanism, or advanced techniques like Differential Privacy under sampling, are used to handle different situations and provide better privacy-accuracy trade-offs.\n",
        "\n",
        "Please note that applying differential privacy involves a deep understanding of data sensitivity, privacy requirements, and how the added noise can affect the usability of the data. Moreover, it's also important to understand how to set appropriate ε and δ values as per the privacy requirements. It's always recommended to refer to detailed textbooks and academic papers on differential privacy or consult with a privacy expert."
      ],
      "metadata": {
        "id": "bhgjk_jKpaEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponential Mechanism\n",
        "\n",
        "Let's go a bit deeper into differential privacy. We'll discuss another type of mechanism for preserving privacy, known as the Exponential Mechanism.\n",
        "\n",
        "**The Exponential Mechanism** is a differential privacy system that selects an output from a set according to a probability distribution. This probability is proportional to the exponential of the utility of the output divided by the privacy budget.\n",
        "\n",
        "The utility of an output is a measure of how useful that output is. Higher utility means that the output is more desirable. By adding randomness in proportion to the utility, the Exponential Mechanism ensures that even if some outputs are more useful than others, an observer cannot be certain about the true data because all outputs have some probability of being chosen.\n",
        "\n",
        "The Exponential Mechanism is often used when you need to select an output from a discrete set of outputs (like selecting the most common age group from a set of age groups).\n",
        "\n",
        "Here is a simple implementation of the Exponential Mechanism in Python. In this example, we calculate the most common age group from a dataset. This code calculates the count of ages in each age group, adds noise to the counts using the Exponential Mechanism, and selects an age group according to the noisy counts. This gives us a differentially private way to find the most common age group.\n",
        "\n",
        "In general, choosing the right differential privacy mechanism and setting the parameters requires understanding the data, the queries, and the privacy requirements. Advanced topics in differential privacy, such as the privacy accountant, moments accountant, and privacy amplification, are often used in practice to manage the privacy budget and provide stronger privacy guarantees.\n",
        "\n",
        "Differential privacy is a complex field of study, and implementing it in a real-world scenario often requires expert knowledge and careful consideration. Always refer to the latest research and standards, and consider consulting with a privacy expert when necessary."
      ],
      "metadata": {
        "id": "9p9pr-uqpreS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Privacy parameter\n",
        "epsilon = 0.5\n",
        "\n",
        "# Our sensitive data\n",
        "ages = np.array([25, 30, 35, 40, 45, 25, 30, 25])\n",
        "\n",
        "# Possible outputs\n",
        "age_groups = np.array([20, 30, 40, 50])\n",
        "\n",
        "# Calculate the utility of each output\n",
        "# The utility is the count of the number of ages in each age group\n",
        "utilities = np.array([np.sum((ages >= group) & (ages < group + 10)) for group in age_groups])\n",
        "\n",
        "# Compute the sensitivity of the query\n",
        "# In this case, it's 1, because adding or removing an individual can change the count by at most 1\n",
        "sensitivity = 1.0\n",
        "\n",
        "# Calculate the probabilities for each output\n",
        "probabilities = np.exp(utilities * epsilon / (2 * sensitivity))\n",
        "\n",
        "# Normalize the probabilities so they sum to 1\n",
        "probabilities /= np.sum(probabilities)\n",
        "\n",
        "# Choose an output according to the calculated probabilities\n",
        "dp_result = np.random.choice(age_groups, p=probabilities)\n",
        "\n",
        "print(f\"Differentially private result: {dp_result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsh-292Cp3Ra",
        "outputId": "3b28aaed-4788-4f46-80e3-e6f666a2dae4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differentially private result: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've seen two types of noise addition mechanisms for differential privacy (Laplace and Exponential), let's delve into a topic that is particularly important when you're performing multiple queries, which is the concept of the Privacy Budget.\n",
        "\n",
        "The privacy budget is a limit on the total amount of information that can be revealed about an individual. In other words, the privacy budget quantifies the privacy loss that an individual is willing to tolerate. The privacy budget is often represented by the symbol ε. The smaller the value of ε, the smaller the privacy loss, and the greater the privacy protection.\n",
        "\n",
        "The concept of privacy budget is important in the context of multiple queries. In differential privacy, every time you make a query against the data, some amount of privacy is 'spent' from the budget. If you make too many queries, you could exhaust the privacy budget, which means that the individuals in the dataset might no longer have their privacy protected.\n",
        "\n",
        "The privacy budget is consumed according to the composition theorem. There are two types of composition: Sequential Composition and Parallel Composition.\n",
        "\n",
        "Sequential Composition: If a series of k algorithms A1, A2, ..., Ak each satisfy ε-DP, then running them sequentially on the same dataset satisfies k*ε-DP. In other words, the privacy losses add up.\n",
        "\n",
        "Parallel Composition: If a series of k algorithms each satisfy ε-DP, and they each operate on disjoint subsets of the data, then the whole sequence also satisfies ε-DP. This means that if each query operates on a different part of the data, the privacy loss does not add up.\n",
        "\n",
        "These principles guide how to manage the privacy budget when making multiple queries.\n",
        "\n",
        "Now, remember the noise addition strategies we discussed earlier? To ensure that your calculations remain within your privacy budget, you would typically add more noise for each subsequent query. However, how much noise to add can be a complex decision, as it can affect the utility of the results. There are different methods for calculating how much noise to add, often involving advanced mathematical and statistical concepts.\n",
        "\n",
        "Overall, implementing differential privacy in a real-world scenario can be complex and requires careful consideration. Always refer to the latest research and standards, and consider consulting with a privacy expert when necessary."
      ],
      "metadata": {
        "id": "5uXt5VQJqEWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced topics\n",
        "\n",
        "You have already learned some key concepts about differential privacy, including privacy budget, sequential and parallel composition, the Laplace and Exponential Mechanisms, and ε-differential privacy and (ε, δ)-differential privacy. Now, let's talk about advanced topics such as Advanced Composition and Privacy Amplification.\n",
        "\n"
      ],
      "metadata": {
        "id": "zP7bc1dvqFyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Advanced Composition Theorems:\n",
        "\n",
        "Advanced Composition Theorems help provide a more refined bound on the overall privacy loss when we conduct multiple computations or queries. The basic composition theorem would suggest that privacy loss compounds linearly with the number of computations, but in reality, the overall privacy loss is generally less than this linear sum.\n",
        "\n",
        "Under the Advanced Composition Theorem, if we have k algorithms that each provide (ε, δ)-differential privacy, then running them sequentially would result in (ε√k log(1/δ), kδ)-differential privacy overall.\n",
        "\n"
      ],
      "metadata": {
        "id": "rC9r5relqSgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Privacy Amplification:\n",
        "\n",
        "Privacy Amplification is the concept that privacy can be enhanced by randomness in the data selection process. In scenarios where we have random sampling, privacy amplification by sampling suggests that differential privacy can be strengthened simply by reducing the sample size. Privacy amplification allows the privacy parameter to scale with the size of the sample rather than the size of the whole database, which means you can obtain a better privacy-accuracy trade-off when you have a large database.\n",
        "\n"
      ],
      "metadata": {
        "id": "bve9J2vJqWWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Post-processing:\n",
        "\n",
        "Differential privacy has an important property called post-processing immunity, which means that any function applied to the output of a differentially private algorithm will also be differentially private. This property ensures that the privacy guarantee is preserved regardless of how the output is later used.\n",
        "\n",
        "Implementing these concepts in Python would largely depend on the specific requirements of your application. Most of these concepts are abstract and don't directly translate into specific Python code, but they guide how you would structure your differential privacy algorithms and how much noise you should add.\n",
        "\n",
        "Remember, differential privacy is a complex and active field of research. Implementing differential privacy in a real-world situation often requires careful analysis and sometimes even new theoretical developments. Always refer to the latest research and standards, and consider consulting with a privacy expert when necessary."
      ],
      "metadata": {
        "id": "WCXeYeXrqZOU"
      }
    }
  ]
}